<h1 align="center">📊 EDA Portfolio using Python & Pandas</h1>

<p align="center">
  <b>A clean and insightful analysis of multiple datasets for better decision-making.</b><br/>
  This project is part of my personal journey into Data science and Machine Learning
</p>

---

## 📌 Overview

This repository contains multiple **Exploratory Data Analysis (EDA) projects** performed on different datasets. Each notebook demonstrates how to explore, visualize, and extract meaningful insights from raw data using Python and `pandas`.  

Key tasks performed include:

- Data cleaning & preprocessing
- Handling missing values
- Encoding categorical variables
- Feature extraction
- Visualizations and trend analysis
- Insights and recommendations

---

## 📁 Project Structure
```
eda-portfolio/
│
├── data/
│ ├── raw/ # Original datasets (Excel, CSV)
│ └── processed/ # Cleaned or processed datasets
│
├── notebooks/
│ ├── ecommerce_furniture_data (EDA1.ipynb)
│
├── requirements.txt # Python libraries needed
├── .gitignore
└── README.md
```

---

## 🔎 Datasets Included

| Dataset | Source | Description |
|---------|--------|-------------|
| **Furniture Sales** | Internship Project (Confidential Dataset) | Transactional sales data including product categories, revenue trends, and customer segments |

*(More datasets will be added in the future)*

---

## 🔧 Analysis Tasks Performed

- ✅ Explored dataset structure & summary statistics  
- ✅ Visualized distributions and trends using Matplotlib/Seaborn  
- ✅ Handled missing values and duplicates  
- ✅ Encoded categorical variables  
- ✅ Extracted new features for deeper insights  
- ✅ Documented insights with Markdown for clarity

---

## 🚀 Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/your-username/eda-portfolio.git
cd eda-portfolio
```
### 2. Insatll Dependencies
```bash
pip install -r requirements.txt
```
### 3. Place raw datasets inside
```bash
# Put all your original datasets (Excel, CSV) here
data/raw/
```
### 4. Processed or cleaned outputs are saved to
```bash
# Cleaned or processed datasets will be stored here
data/processed/
```
### 5. Open notebooks
```bash
# Launch Jupyter Notebook to view and run analysis
jupyter notebook notebooks/
```
### 🛠 Tools Used

- Python 3.x

- pandas, numpy

- Matplotlib, Seaborn

- Jupyter Notebook

### 🧠 Learnings

- Importance of data exploration before modeling

- Handling missing and inconsistent data

- Understanding trends, correlations, and distributions

- Presenting insights in a clear, reproducible format
  
### 📈 Future Improvements

- Add more datasets from different domains (Finance, Healthcare, Retail, etc.)

- Automate EDA report generation using Python scripts

- Explore interactive visualization with Plotly/Streamlit

- Modularize notebooks for reusable analysis pipelines

### 👥 Contributors
| Name              | GitHub Profile                                   |
| ----------------- | ------------------------------------------------ |
| Vernon Isaac Alva | [@Vernon-codes](https://github.com/Vernon-codes) |

### 🙏 Acknowledgements

- Datasets source from Internship(Confidential)

- Inspired by data science best practices from online tutorials and guides

- Built using open-source libraries like pandas, numpy, matplotlib, and seaborn

### 📄 License

- This project is licensed under the MIT License

## 📬 Contact
If you have any questions, feel free to reach out:

- GitHub: https://github.com/Vernon-codes/
- LinkedIn: https://www.linkedin.com/in/vernon-alva-8677b523a/  
- Email: codewithvernon@gmail.com

## 🙌 Contributing
Contributions are welcome!  
Please fork this repository, make your changes, and submit a pull request.

## ⭐ Support
If you found this project helpful, please consider giving it a star on GitHub ⭐




